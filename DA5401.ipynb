{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ec70330",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* This notebook builds a score prediction model by combining embeddings, anomaly signals, match probabilities, and coherence measures. \n",
    "* The goal is to extract meaningful interactions and train a regressor that handles clustered/bimodal target behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52150bd",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a97c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neural_network import MLPClassifier # The \"Scikit-Learn Neural Net\"\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58bd221",
   "metadata": {},
   "source": [
    "# Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8073e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "test_ids = np.load('test_ids.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b6ccd5",
   "metadata": {},
   "source": [
    "# Load the preprocessed embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d522a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Load embeddings and targets\n",
    "# ============================\n",
    "y = np.load('train_scores.npy')\n",
    "\n",
    "# Load ALL embeddings (System included)\n",
    "train_metric = np.load(\"train_metric_embedding.npy\").astype(np.float32)\n",
    "train_resp   = np.load(\"train_response_embedding.npy\").astype(np.float32)\n",
    "train_user   = np.load(\"train_user_prompt_embedding.npy\").astype(np.float32)\n",
    "train_sys = np.load(\"train_system_prompt_embedding.npy\").astype(np.float32)\n",
    "\n",
    "test_metric = np.load(\"test_metric_embedding.npy\").astype(np.float32)\n",
    "test_resp   = np.load(\"test_response_embedding.npy\").astype(np.float32)\n",
    "test_user   = np.load(\"test_user_prompt_embedding.npy\").astype(np.float32)\n",
    "test_sys = np.load(\"test_system_prompt_embedding.npy\").astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a71693f",
   "metadata": {},
   "source": [
    "# Feature Preparation\n",
    "\n",
    "* We combine system, user, and response embeddings into a unified text feature space. \n",
    "* This helps capture conversation-level semantics and ensures each training row reflects the full context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c0b5f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Vector Shape: (5000, 2304)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. PREPARE FEATURES ---\n",
    "# Concat System + User + Response\n",
    "X_text_train = np.hstack([train_sys, train_user, train_resp])\n",
    "X_text_test  = np.hstack([test_sys,  test_user,  test_resp])\n",
    "X_metric_train = train_metric\n",
    "X_metric_test  = test_metric\n",
    "\n",
    "print(f\"Text Vector Shape: {X_text_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4159924",
   "metadata": {},
   "source": [
    "# Signal Engineering (F1, F2, F3)\n",
    "\n",
    "* Three complementary signals—anomaly detection, metric–text matching, and semantic coherence—capture different behavioral dimensions. \n",
    "* These signals help the model distinguish strong vs weak metric associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41623480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting F1 (Isolation Forest)...\n",
      "Fitting F2 (MLP Matcher)...\n"
     ]
    }
   ],
   "source": [
    "# --- 4. DEFINE NEW FEATURES (F1, F2, F3) ---\n",
    "\n",
    "# F1: Anomaly Detection uaing Isolation Forest\n",
    "# Detects outliers in the text embedding space.\n",
    "print(\"Fitting F1 (Isolation Forest)...\")\n",
    "iso_model = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination='auto',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "iso_model.fit(X_text_train)\n",
    "\n",
    "# F2: Metric–Text Match Predictor (MLPClassifier)\n",
    "# Uses a small neural network to learn whether a metric embedding\n",
    "# belongs to a given text embedding.\n",
    "print(\"Fitting F2 (MLP Matcher)...\")\n",
    "\n",
    "# 1. Positives (Real Pairs)\n",
    "X_positives = np.hstack([X_metric_train, X_text_train])\n",
    "y_positives = np.ones(len(X_positives))\n",
    "\n",
    "# 2. Fake (negative) pairs – metric with mismatched text\n",
    "# Using 2× shuffled negatives makes the classifier stricter.\n",
    "rng = np.random.RandomState(42)\n",
    "idx_shuf1 = rng.permutation(len(X_text_train))\n",
    "idx_shuf2 = rng.permutation(len(X_text_train))\n",
    "\n",
    "X_neg1 = np.hstack([X_metric_train, X_text_train[idx_shuf1]])\n",
    "X_neg2 = np.hstack([X_metric_train, X_text_train[idx_shuf2]])\n",
    "y_neg = np.zeros(len(X_neg1) + len(X_neg2))\n",
    "\n",
    "X_matcher = np.vstack([X_positives, X_neg1, X_neg2])\n",
    "y_matcher = np.concatenate([y_positives, y_neg])\n",
    "\n",
    "# MLP: Hidden layers project the high-dim embeddings. \n",
    "\n",
    "matcher = MLPClassifier(\n",
    "    hidden_layer_sizes=(512, 256), # Layers to compress info\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    batch_size=256,\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=150, # 50 epochs is usually enough\n",
    "    random_state=42\n",
    ")\n",
    "matcher.fit(X_matcher, y_matcher)\n",
    "\n",
    "# # F3: Cosine Similarity (Semantic Alignment)\n",
    "def get_s3(u, r):\n",
    "    num = np.sum(u * r, axis=1)\n",
    "    den = np.linalg.norm(u, axis=1) * np.linalg.norm(r, axis=1)\n",
    "    return num / (den + 1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ae4c7a",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "Synthetic mismatched examples are created to teach the model how low‑quality pairs behave. This helps LightGBM avoid over‑predicting and improves separation between good and bad samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dc63baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating New Features for improved model accuracy\n",
      "Synthetic Data Creation Started...\n",
      "Data Creation Complete...\n"
     ]
    }
   ],
   "source": [
    "# --- 5. DATA AUGMENTATION & NEW FEATURE GENERATION ---\n",
    "print(\"Generating New Features for improved model accuracy\")\n",
    "\n",
    "# We create additional training samples so that the regressor\n",
    "# learns how \"bad\" or mismatched metric–text pairs behave.\n",
    "# This helps the model avoid over-predicting scores.\n",
    "# Number of synthetic samples = size of original training set\n",
    "\n",
    "n_aug = len(y)\n",
    "idx_aug = rng.permutation(len(y))\n",
    "\n",
    "# --- Synthetic (Negative) Data Construction ---\n",
    "# We keep metric embeddings the same but shuffle text/user/response embeddings.\n",
    "# This breaks the true pairing and produces intentionally incorrect combinations.\n",
    "\n",
    "print('Synthetic Data Creation Started...')\n",
    "aug_metric = X_metric_train\n",
    "aug_text   = X_text_train[idx_aug]\n",
    "aug_user   = train_user[idx_aug]\n",
    "aug_resp   = train_resp[idx_aug]\n",
    "print('Data Creation Complete...')\n",
    "\n",
    "# Synthetic scores: force them to be low (0.0 to 3.5).\n",
    "# This trains LightGBM to recognize and penalize mismatches.\n",
    "\n",
    "aug_y      = rng.uniform(0.0, 3.5, size=n_aug)\n",
    "\n",
    "# --- Merge Real + Synthetic Data ---\n",
    "# Stack real data with synthetic \"bad\" pairs to create the final training set.\n",
    "X_text_total = np.vstack([X_text_train, aug_text])\n",
    "X_metric_total = np.vstack([X_metric_train, aug_metric])\n",
    "train_user_total = np.vstack([train_user, aug_user])\n",
    "train_resp_total = np.vstack([train_resp, aug_resp])\n",
    "y_total = np.concatenate([y, aug_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e3c5db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- COMPUTE Features ---\n",
    "\n",
    "# F1: Anomaly\n",
    "# Isolation Forest outputs a raw anomaly value (higher = more normal, lower = more anomalous).\n",
    "# We compute this score for both augmented training data and test data.\n",
    "s1_raw_tr = iso_model.decision_function(X_text_total)\n",
    "s1_raw_te = iso_model.decision_function(X_text_test)\n",
    "sc1 = MinMaxScaler()\n",
    "S1_train = sc1.fit_transform(s1_raw_tr.reshape(-1,1)).flatten()\n",
    "S1_test  = sc1.transform(s1_raw_te.reshape(-1,1)).flatten()\n",
    "\n",
    "# F2: Matcher (Probability)\n",
    "# We combine metric and text embeddings so the matcher can evaluate if they belong together.\n",
    "X_concat_tr = np.hstack([X_metric_total, X_text_total])\n",
    "X_concat_te = np.hstack([X_metric_test, X_text_test])\n",
    "S2_train = matcher.predict_proba(X_concat_tr)[:, 1]\n",
    "S2_test  = matcher.predict_proba(X_concat_te)[:, 1]\n",
    "\n",
    "# F3: Coherence\n",
    "# Computes cosine similarity between user embedding and response embedding.\n",
    "# Higher value → more semantically aligned interaction.\n",
    "s3_raw_tr = get_s3(train_user_total, train_resp_total)\n",
    "s3_raw_te = get_s3(test_user, test_resp)\n",
    "sc3 = MinMaxScaler()\n",
    "S3_train = sc3.fit_transform(s3_raw_tr.reshape(-1,1)).flatten()\n",
    "S3_test  = sc3.transform(s3_raw_te.reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeebecb",
   "metadata": {},
   "source": [
    "# Final Regression Model\n",
    "* Interaction features (S1×S2×S3) highlight cluster boundaries, and LightGBM is tuned to detect multimodal patterns. \n",
    "* This produces stable final scores aligned with high‑quality matching behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca570a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM...\n"
     ]
    }
   ],
   "source": [
    "# --- 6. FINAL REGRESSION ---\n",
    "def get_feats(s1, s2, s3):\n",
    "    df = pd.DataFrame({\"S1\": s1, \"S2\": s2, \"S3\": s3})\n",
    "    \n",
    "    # Create interaction terms between signals.\n",
    "    # These help the model capture *joint patterns*:\n",
    "    # - S1xS2: anomaly + match strength\n",
    "    # - S2xS3: match strength + semantic coherence\n",
    "    # - S1xS3: anomaly + coherence\n",
    "    # - All: full 3-way interaction capturing \"perfect agreement\"\n",
    "    # Strong interactions to isolate the \"Perfect\" cluster (High S1*S2*S3)\n",
    "    \n",
    "    df[\"S1xS2\"] = df.S1 * df.S2\n",
    "    df[\"S2xS3\"] = df.S2 * df.S3\n",
    "    df[\"S1xS3\"] = df.S1 * df.S3\n",
    "    df[\"All\"]   = df.S1 * df.S2 * df.S3\n",
    "    return df\n",
    "\n",
    "# Build training and test feature matrices\n",
    "\n",
    "df_train = get_feats(S1_train, S2_train, S3_train)\n",
    "df_test  = get_feats(S1_test, S2_test, S3_test)\n",
    "\n",
    "# -------------------- Sample Weighting --------------------\n",
    "# Count how many samples occur in each rounded score value.\n",
    "# Rare score levels get higher weight so the model does not ignore them.\n",
    "counts = pd.Series(np.round(y_total)).value_counts()\n",
    "\n",
    "# Weight = inverse frequency of that rounded score\n",
    "# Helps LightGBM treat underrepresented score levels fairly.\n",
    "weights = pd.Series(np.round(y_total)).map(lambda x: 1.0 / counts.get(x, 1.0)).values\n",
    "\n",
    "print(\"Training LightGBM...\")\n",
    "dtrain = lgb.Dataset(df_train, label=y_total, weight=weights)\n",
    "\n",
    "# Parameters optimized to separate clusters rather than smooth the mean\n",
    "# -------------------- Model Parameters --------------------\n",
    "# Settings tuned to help LightGBM separate clusters (low vs high scores)\n",
    "# instead of treating the target as a smooth regression line.\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"learning_rate\": 0.05, \n",
    "    \"num_leaves\": 63,        # Higher leaves = more complex splits (good for bimodal)\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"min_data_in_leaf\": 10,  # Allow smaller clusters (peaks)\n",
    "    \"seed\": 42,\n",
    "    \"verbosity\": -1\n",
    "}\n",
    "\n",
    "\n",
    "# Train the final LightGBM regressor\n",
    "model = lgb.train(params, dtrain, num_boost_round=1200)\n",
    "pred_test = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a29ed3",
   "metadata": {},
   "source": [
    "# Saving the CSV in the required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a3aa411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission Saved.\n",
      "count    3638.000000\n",
      "mean        5.636960\n",
      "std         3.432238\n",
      "min         0.000000\n",
      "25%         1.854232\n",
      "50%         7.080926\n",
      "75%         8.881930\n",
      "max        10.000000\n",
      "Name: score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- 8. EXPORT (No Manual Scaling) ---\n",
    "final_df = pd.DataFrame({'ids': test_ids, 'score': pred_test})\n",
    "\n",
    "# Optional: Clip to valid range just to be safe (0-10), but no shifting\n",
    "final_df['score'] = np.clip(final_df['score'], 0, 10)\n",
    "\n",
    "final_df.to_csv(\"stacking_submission_newest.csv\", index=False)\n",
    "\n",
    "print(\"Submission Saved.\")\n",
    "print(final_df['score'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b817d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
